# -*- coding: utf-8 -*-
"""T5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kaK7mrBVI7wnSfSx8A3joKlyT2Yq9xvC

## T5 MODEL

"""

import nltk
nltk.download("punkt", quiet=True)
from sklearn.metrics import classification_report
import numpy as np
#import tensorflow as tf
import random as python_random
import datasets

from transformers import (
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    DataCollatorForSeq2Seq,
)


# Make reproducible as much as possible
np.random.seed(1234)
#tf.random.set_seed(1234)
python_random.seed(1234)
seed = 1234


# Global vars used in the project -> used in functions as well
tokenizer = AutoTokenizer.from_pretrained('t5-base')
model = AutoModelForSeq2SeqLM.from_pretrained('t5-base')

# Values calculated in the data-explore notebook
max_input_length = 125
max_output_length = 193


def preprocess(data):
    """
    Function used to preprocess the input and output for the model.
    The input gets the special [SEP] token and the output gets a space added.
    """
    return {
        "input": data['premise'] + '</s>' + data['hypothesis'],
        "output": str(data['label']) + ' ' + data['explanation_1'],
    }

# adapted from:
# https://github.com/huggingface/transformers/blob/main/examples/pytorch/summarization/run_summarization.py
def batch_tokenize_preprocess(batch, tokenizer, max_source_length, max_target_length):
    """
    Function used to tokenize our inputs in batches, saves RAM usage/
    """
    input_tokenized = tokenizer(
        batch['input'], padding="max_length", truncation=True, max_length=max_source_length
    )

    output_tokenized = tokenizer(
        batch['output'], padding="max_length", truncation=True, max_length=max_target_length
    )

    batch = {k: v for k, v in input_tokenized.items()}

    # Ignore padding in the loss
    batch["labels"] = [
        [-100 if token == tokenizer.pad_token_id else token for token in l]
        for l in output_tokenized["input_ids"]
    ]
    return batch



# Adapted from:
# https://github.com/huggingface/transformers/blob/main/examples/pytorch/summarization/run_summarization.py
rouge_metric = datasets.load_metric("rouge")

def postprocess_text(preds, labels):
    """
    Function that is used to reformat the data in a way that we can create a classification report
    and obtain the ROUGE scores
    """
    # rougeLSum expects newline after each sentence
    preds = ["\n".join(nltk.sent_tokenize(pred.strip())) for pred in preds]
    labels = ["\n".join(nltk.sent_tokenize(label.strip())) for label in labels]

    # Code that is needed to get the classification report
    nli_preds, nli_golds = [], []
    correct_labels = ['0', '1', '2']
    for idx, l in enumerate(labels):
      n = preds[idx][0]
      if n in correct_labels:
        nli_preds.append(n)
      else:
        nli_preds.append('NA')
      nli_golds.append(l[0])

    return preds, labels, nli_preds, nli_golds


def compute_metrics(eval_preds):
    """
    Function that is used to obtain the ROUGE and classifcation report
    """
   # preds = the output of the model that was generated
   # labels = gold labels from the validation set
    print('Started compute metrics')
    preds, labels = eval_preds

    if isinstance(preds, tuple):
        preds = preds[0]

    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    # take care of padding
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)

    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Some simple post-processing
    decoded_preds, decoded_labels, nli_preds, nli_golds = postprocess_text(decoded_preds, decoded_labels)

    print(classification_report(nli_golds, nli_preds))
    
    rouge = rouge_metric.compute(
        predictions=decoded_preds, references=decoded_labels, use_stemmer=True
    )

    # Extract a few results from ROUGE
    rouge = {key: value.mid.fmeasure * 100 for key, value in rouge.items()}

    prediction_lens = [
        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds
    ]
    rouge["gen_len"] = np.mean(prediction_lens)
    rouge = {k: round(v, 4) for k, v in rouge.items()}
    return rouge

def prepare_test(td):
    """
    Function that is used to tokenize the test data
    """
    input, output = td['input'], td['output']

    tok_inputs = tokenizer(
      input,
      padding="max_length",
      truncation=True,
      max_length=max_input_length,
      return_tensors="pt",
    )

    tok_outputs = tokenizer(
      output,
      padding="max_length",
      truncation=True,
      max_length=max_output_length,
      return_tensors="pt",
    )

    return tok_inputs, tok_outputs

def generate_predictions(model, input, output):
    """
    Function that is used to obtain the test predictions and metrics
    """
    # Obtain iput ids and attention mask
    input_ids = input.input_ids.to(model.device)
    attention_mask = input.attention_mask.to(model.device)
    # Use the trained model to generate outputs using the input ids and AM
    test_outputs = model.generate(input_ids, attention_mask=attention_mask)

    print('Test Rouge scores:', compute_metrics((test_outputs.cpu(), output.input_ids.cpu())))

    # Coverts output_ids back to string representation using the decode
    output_str = tokenizer.batch_decode(test_outputs, skip_special_tokens=True)

    return test_outputs, output_str


def main():
    """
    Main function of the script!
    """
    # Loading the E-SNLI dataset from HuggingFace
    # 100000, 5000, 5000 -> should be full van and test in the end
    train = datasets.load_dataset('esnli', split='train[:100000]').shuffle(seed)
    val = datasets.load_dataset('esnli', split='validation').shuffle(seed)
    test = datasets.load_dataset('esnli', split='test').shuffle(seed)

    train_dict = train.map(preprocess,
                           remove_columns=['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2',
                                           'explanation_3'])

    val_dict = val.map(preprocess, remove_columns=['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2',
                                                   'explanation_3'])

    test_dict = test.map(preprocess, remove_columns=['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2',
                                                     'explanation_3'])

    train_data = train_dict.map(
        lambda batch: batch_tokenize_preprocess(
            batch, tokenizer, max_input_length, max_output_length
        ),
        batched=True,
        remove_columns=train_dict.column_names,
    )

    val_data = val_dict.map(
        lambda batch: batch_tokenize_preprocess(
            batch, tokenizer, max_input_length, max_output_length
        ),
        batched=True,
        remove_columns=val_dict.column_names,
    )


    # Using the seq2seq training flow from huggingface.
    # We weren't quite sure how to create a training flow manually as this is the first
    # time working with a encoder/decoder model.
    training_args = Seq2SeqTrainingArguments(
        output_dir='fine-tune-results',
        num_train_epochs=1,
        do_train=True,
        do_eval=True,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        learning_rate=2e-05,
        weight_decay=0.01,
        label_smoothing_factor=0.1,
        predict_with_generate=True,
        logging_dir="logs",
        logging_steps=1000,
        save_total_limit=3,
    )

    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_data,
        eval_dataset=val_data,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )

    # labels: 0 = entailment, 1 = neutral, 2 = contradiction
    # Training the model
    trainer.train()

    # Evaluating on our validation set
    trainer.evaluate()

    # Evaluate on our test set
    test_in, test_out = prepare_test(test_dict)
    predictions_after_tuning = generate_predictions(model, test_in, test_out)[1]

    # Printing the first 10 test predictions
    for idx, i in enumerate(predictions_after_tuning[:10]):
        print(test['premise'][idx])
        print(test['hypothesis'][idx])
        print(i)
        print('\n')


if __name__ == "__main__":
    main()
