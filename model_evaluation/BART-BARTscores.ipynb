{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "BART-BARTscores.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "mount_file_id": "1Phah-5ppwhNq4bn9GoQ1bH3J_csX8XD9",
   "authorship_tag": "ABX9TyMIVKKluTuk4cJUouAXlWJ8"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "3978d35ad7c54feb92a2d18a9944a52b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f0982a2529b248ec957945ae03540700",
       "IPY_MODEL_cac27abaff164e3a93a33d40a4eec44a",
       "IPY_MODEL_fa3cd2cddae1403886f754c6da7c829e"
      ],
      "layout": "IPY_MODEL_9237c454d8bf4fc7b20147b0eba17494"
     }
    },
    "f0982a2529b248ec957945ae03540700": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_032e8be9944d404ba8a351f51bbe6559",
      "placeholder": "​",
      "style": "IPY_MODEL_fee08e1c92fa4aa096f07cd3df8ce1d9",
      "value": "100%"
     }
    },
    "cac27abaff164e3a93a33d40a4eec44a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c24aa53cd0b44adbabda2303229eeb1",
      "max": 9824,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4776ef227d2a4783b7a3c69cfe92b55b",
      "value": 9824
     }
    },
    "fa3cd2cddae1403886f754c6da7c829e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b55b2ed087cc4311b3a8e9e983f375f3",
      "placeholder": "​",
      "style": "IPY_MODEL_66383c727a6d4df4b675418cb527accc",
      "value": " 9824/9824 [00:05&lt;00:00, 1973.29ex/s]"
     }
    },
    "9237c454d8bf4fc7b20147b0eba17494": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "032e8be9944d404ba8a351f51bbe6559": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fee08e1c92fa4aa096f07cd3df8ce1d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6c24aa53cd0b44adbabda2303229eeb1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4776ef227d2a4783b7a3c69cfe92b55b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b55b2ed087cc4311b3a8e9e983f375f3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66383c727a6d4df4b675418cb527accc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a95a6db6cb234c3f85df5b8c996adbc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e8b9b20b8e7847d4b69ee2d6b0b2abf4",
       "IPY_MODEL_d1545ce181c343fab54c8759d6dec904",
       "IPY_MODEL_dcebf48949344e9887c15c5932c0adf4"
      ],
      "layout": "IPY_MODEL_6d20ffb69e604d1aa1212a64f34a37cc"
     }
    },
    "e8b9b20b8e7847d4b69ee2d6b0b2abf4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b894e92d543440597eb0c71b5e8f9f7",
      "placeholder": "​",
      "style": "IPY_MODEL_f383c8050def4e4f8d6035df3932a0dc",
      "value": "100%"
     }
    },
    "d1545ce181c343fab54c8759d6dec904": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd76a1ca3cf8408fa210a4e7bbe79e9f",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_038ebf38519b4570b0fe086e4e9408a4",
      "value": 10
     }
    },
    "dcebf48949344e9887c15c5932c0adf4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7249ccd208be47088d38b9f704c543e2",
      "placeholder": "​",
      "style": "IPY_MODEL_31fee377404f45889bff6441f9f6a254",
      "value": " 10/10 [00:12&lt;00:00,  1.31s/ba]"
     }
    },
    "6d20ffb69e604d1aa1212a64f34a37cc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b894e92d543440597eb0c71b5e8f9f7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f383c8050def4e4f8d6035df3932a0dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bd76a1ca3cf8408fa210a4e7bbe79e9f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "038ebf38519b4570b0fe086e4e9408a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7249ccd208be47088d38b9f704c543e2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31fee377404f45889bff6441f9f6a254": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook used to calculate the BartScores for BART\n",
    "\n",
    "please note: this notebook only works on pre-trained models and the path to this model has to specified."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: rouge_score in /usr/local/lib/python3.7/dist-packages (0.0.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.15.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.21.5)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.0.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge_score) (3.2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentencepiece torch datasets\n",
    "!pip install rouge_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "import datasets\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BartTokenizer, BartForConditionalGeneration\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import traceback\n",
    "from typing import List\n",
    "\n",
    "import pickle as pickle_rick\n",
    "\n",
    "#TODO: Set your path to a pre-trained model here (BART\n",
    "#TODO: change path to match locally\n",
    "trained_bart = f\"\"\n",
    "# Global vars used in the project -> used in functions as well\n",
    "tokenizer = AutoTokenizer.from_pretrained(trained_bart)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(trained_bart)\n"
   ],
   "metadata": {
    "id": "Ojjxl0qEzJwu",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1649594746012,
     "user_tz": -120,
     "elapsed": 3747,
     "user": {
      "displayName": "F.A. Leistra",
      "userId": "17579912925770168307"
     }
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class BARTScorer:\n",
    "    def __init__(self, device='cuda:0', max_length=1024, checkpoint='facebook/bart-large-cnn'):\n",
    "        # Set up model\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(checkpoint)\n",
    "        self.model = BartForConditionalGeneration.from_pretrained(checkpoint)\n",
    "        self.model.eval()\n",
    "        self.model.to(device)\n",
    "\n",
    "        # Set up loss\n",
    "        self.loss_fct = nn.NLLLoss(reduction='none', ignore_index=self.model.config.pad_token_id)\n",
    "        self.lsm = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def load(self, path=None):\n",
    "        \"\"\" Load model from paraphrase finetuning \"\"\"\n",
    "        if path is None:\n",
    "            path = 'models/bart.pth'\n",
    "        self.model.load_state_dict(torch.load(path, map_location=self.device))\n",
    "\n",
    "    def score(self, srcs, tgts, batch_size=4):\n",
    "        \"\"\" Score a batch of examples \"\"\"\n",
    "        score_list = []\n",
    "        for i in range(0, len(srcs), batch_size):\n",
    "            src_list = srcs[i: i + batch_size]\n",
    "            tgt_list = tgts[i: i + batch_size]\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    encoded_src = self.tokenizer(\n",
    "                        src_list,\n",
    "                        max_length=self.max_length,\n",
    "                        truncation=True,\n",
    "                        padding=True,\n",
    "                        return_tensors='pt'\n",
    "                    )\n",
    "                    encoded_tgt = self.tokenizer(\n",
    "                        tgt_list,\n",
    "                        max_length=self.max_length,\n",
    "                        truncation=True,\n",
    "                        padding=True,\n",
    "                        return_tensors='pt'\n",
    "                    )\n",
    "                    src_tokens = encoded_src['input_ids'].to(self.device)\n",
    "                    src_mask = encoded_src['attention_mask'].to(self.device)\n",
    "\n",
    "                    tgt_tokens = encoded_tgt['input_ids'].to(self.device)\n",
    "                    tgt_mask = encoded_tgt['attention_mask']\n",
    "                    tgt_len = tgt_mask.sum(dim=1).to(self.device)\n",
    "\n",
    "                    output = self.model(\n",
    "                        input_ids=src_tokens,\n",
    "                        attention_mask=src_mask,\n",
    "                        labels=tgt_tokens\n",
    "                    )\n",
    "                    logits = output.logits.view(-1, self.model.config.vocab_size)\n",
    "                    loss = self.loss_fct(self.lsm(logits), tgt_tokens.view(-1))\n",
    "                    loss = loss.view(tgt_tokens.shape[0], -1)\n",
    "                    loss = loss.sum(dim=1) / tgt_len\n",
    "                    curr_score_list = [-x.item() for x in loss]\n",
    "                    score_list += curr_score_list\n",
    "\n",
    "            except RuntimeError:\n",
    "                traceback.print_exc()\n",
    "                print(f'source: {src_list}')\n",
    "                print(f'target: {tgt_list}')\n",
    "                exit(0)\n",
    "        return score_list\n",
    "\n",
    "    def multi_ref_score(self, srcs, tgts: List[List[str]], agg=\"mean\", batch_size=4):\n",
    "        # Assert we have the same number of references\n",
    "        ref_nums = [len(x) for x in tgts]\n",
    "        if len(set(ref_nums)) > 1:\n",
    "            raise Exception(\"You have different number of references per test sample.\")\n",
    "\n",
    "        ref_num = len(tgts[0])\n",
    "        score_matrix = []\n",
    "        for i in range(ref_num):\n",
    "            curr_tgts = [x[i] for x in tgts]\n",
    "            scores = self.score(srcs, curr_tgts, batch_size)\n",
    "            score_matrix.append(scores)\n",
    "        if agg == \"mean\":\n",
    "            score_list = np.mean(score_matrix, axis=0)\n",
    "        elif agg == \"max\":\n",
    "            score_list = np.max(score_matrix, axis=0)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return list(score_list)\n",
    "\n",
    "    def test(self, batch_size=3):\n",
    "        \"\"\" Test \"\"\"\n",
    "        src_list = [\n",
    "            'This is a very good idea. Although simple, but very insightful.',\n",
    "            'Can I take a look?',\n",
    "            'Do not trust him, he is a liar.'\n",
    "        ]\n",
    "\n",
    "        tgt_list = [\n",
    "            \"That's stupid.\",\n",
    "            \"What's the problem?\",\n",
    "            'He is trustworthy.'\n",
    "        ]\n",
    "\n",
    "        print(self.score(src_list, tgt_list, batch_size))"
   ],
   "metadata": {
    "id": "3OdxlNyqEgc4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1649594746589,
     "user_tz": -120,
     "elapsed": 588,
     "user": {
      "displayName": "F.A. Leistra",
      "userId": "17579912925770168307"
     }
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mBARTScorer\u001B[39;00m:\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda:0\u001B[39m\u001B[38;5;124m'\u001B[39m, max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1024\u001B[39m, checkpoint\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfacebook/bart-large-cnn\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m      3\u001B[0m         \u001B[38;5;66;03m# Set up model\u001B[39;00m\n\u001B[1;32m      4\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice \u001B[38;5;241m=\u001B[39m device\n",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36mBARTScorer\u001B[0;34m()\u001B[0m\n\u001B[1;32m     66\u001B[0m             exit(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m score_list\n\u001B[0;32m---> 69\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmulti_ref_score\u001B[39m(\u001B[38;5;28mself\u001B[39m, srcs, tgts: \u001B[43mList\u001B[49m[List[\u001B[38;5;28mstr\u001B[39m]], agg\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmean\u001B[39m\u001B[38;5;124m\"\u001B[39m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m):\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;66;03m# Assert we have the same number of references\u001B[39;00m\n\u001B[1;32m     71\u001B[0m     ref_nums \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mlen\u001B[39m(x) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m tgts]\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mset\u001B[39m(ref_nums)) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "\u001B[0;31mNameError\u001B[0m: name 'List' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "bart_scorer = BARTScorer(checkpoint=trained_bart)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OwoBz66tAyb2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1649594750989,
     "user_tz": -120,
     "elapsed": 4402,
     "user": {
      "displayName": "F.A. Leistra",
      "userId": "17579912925770168307"
     }
    },
    "outputId": "723613f7-e3ed-4b1a-812a-2d44b445dc15"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "3978d35ad7c54feb92a2d18a9944a52b",
      "f0982a2529b248ec957945ae03540700",
      "cac27abaff164e3a93a33d40a4eec44a",
      "fa3cd2cddae1403886f754c6da7c829e",
      "9237c454d8bf4fc7b20147b0eba17494",
      "032e8be9944d404ba8a351f51bbe6559",
      "fee08e1c92fa4aa096f07cd3df8ce1d9",
      "6c24aa53cd0b44adbabda2303229eeb1",
      "4776ef227d2a4783b7a3c69cfe92b55b",
      "b55b2ed087cc4311b3a8e9e983f375f3",
      "66383c727a6d4df4b675418cb527accc",
      "a95a6db6cb234c3f85df5b8c996adbc1",
      "e8b9b20b8e7847d4b69ee2d6b0b2abf4",
      "d1545ce181c343fab54c8759d6dec904",
      "dcebf48949344e9887c15c5932c0adf4",
      "6d20ffb69e604d1aa1212a64f34a37cc",
      "7b894e92d543440597eb0c71b5e8f9f7",
      "f383c8050def4e4f8d6035df3932a0dc",
      "bd76a1ca3cf8408fa210a4e7bbe79e9f",
      "038ebf38519b4570b0fe086e4e9408a4",
      "7249ccd208be47088d38b9f704c543e2",
      "31fee377404f45889bff6441f9f6a254"
     ]
    },
    "id": "6rM_VFDBxUVM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1649601488164,
     "user_tz": -120,
     "elapsed": 6737179,
     "user": {
      "displayName": "F.A. Leistra",
      "userId": "17579912925770168307"
     }
    },
    "outputId": "5eb8bb29-e587-49ac-dc32-e074ef570218"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset esnli (/root/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/9824 [00:00<?, ?ex/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3978d35ad7c54feb92a2d18a9944a52b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a95a6db6cb234c3f85df5b8c996adbc1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running batch: 1 of total 307\n",
      "Running batch: 2 of total 307\n",
      "Running batch: 3 of total 307\n",
      "Running batch: 4 of total 307\n",
      "Running batch: 5 of total 307\n",
      "Running batch: 6 of total 307\n",
      "Running batch: 7 of total 307\n",
      "Running batch: 8 of total 307\n",
      "Running batch: 9 of total 307\n",
      "Running batch: 10 of total 307\n",
      "Running batch: 11 of total 307\n",
      "Running batch: 12 of total 307\n",
      "Running batch: 13 of total 307\n",
      "Running batch: 14 of total 307\n",
      "Running batch: 15 of total 307\n",
      "Running batch: 16 of total 307\n",
      "Running batch: 17 of total 307\n",
      "Running batch: 18 of total 307\n",
      "Running batch: 19 of total 307\n",
      "Running batch: 20 of total 307\n",
      "Running batch: 21 of total 307\n",
      "Running batch: 22 of total 307\n",
      "Running batch: 23 of total 307\n",
      "Running batch: 24 of total 307\n",
      "Running batch: 25 of total 307\n",
      "Running batch: 26 of total 307\n",
      "Running batch: 27 of total 307\n",
      "Running batch: 28 of total 307\n",
      "Running batch: 29 of total 307\n",
      "Running batch: 30 of total 307\n",
      "Running batch: 31 of total 307\n",
      "Running batch: 32 of total 307\n",
      "Running batch: 33 of total 307\n",
      "Running batch: 34 of total 307\n",
      "Running batch: 35 of total 307\n",
      "Running batch: 36 of total 307\n",
      "Running batch: 37 of total 307\n",
      "Running batch: 38 of total 307\n",
      "Running batch: 39 of total 307\n",
      "Running batch: 40 of total 307\n",
      "Running batch: 41 of total 307\n",
      "Running batch: 42 of total 307\n",
      "Running batch: 43 of total 307\n",
      "Running batch: 44 of total 307\n",
      "Running batch: 45 of total 307\n",
      "Running batch: 46 of total 307\n",
      "Running batch: 47 of total 307\n",
      "Running batch: 48 of total 307\n",
      "Running batch: 49 of total 307\n",
      "Running batch: 50 of total 307\n",
      "Running batch: 51 of total 307\n",
      "Running batch: 52 of total 307\n",
      "Running batch: 53 of total 307\n",
      "Running batch: 54 of total 307\n",
      "Running batch: 55 of total 307\n",
      "Running batch: 56 of total 307\n",
      "Running batch: 57 of total 307\n",
      "Running batch: 58 of total 307\n",
      "Running batch: 59 of total 307\n",
      "Running batch: 60 of total 307\n",
      "Running batch: 61 of total 307\n",
      "Running batch: 62 of total 307\n",
      "Running batch: 63 of total 307\n",
      "Running batch: 64 of total 307\n",
      "Running batch: 65 of total 307\n",
      "Running batch: 66 of total 307\n",
      "Running batch: 67 of total 307\n",
      "Running batch: 68 of total 307\n",
      "Running batch: 69 of total 307\n",
      "Running batch: 70 of total 307\n",
      "Running batch: 71 of total 307\n",
      "Running batch: 72 of total 307\n",
      "Running batch: 73 of total 307\n",
      "Running batch: 74 of total 307\n",
      "Running batch: 75 of total 307\n",
      "Running batch: 76 of total 307\n",
      "Running batch: 77 of total 307\n",
      "Running batch: 78 of total 307\n",
      "Running batch: 79 of total 307\n",
      "Running batch: 80 of total 307\n",
      "Running batch: 81 of total 307\n",
      "Running batch: 82 of total 307\n",
      "Running batch: 83 of total 307\n",
      "Running batch: 84 of total 307\n",
      "Running batch: 85 of total 307\n",
      "Running batch: 86 of total 307\n",
      "Running batch: 87 of total 307\n",
      "Running batch: 88 of total 307\n",
      "Running batch: 89 of total 307\n",
      "Running batch: 90 of total 307\n",
      "Running batch: 91 of total 307\n",
      "Running batch: 92 of total 307\n",
      "Running batch: 93 of total 307\n",
      "Running batch: 94 of total 307\n",
      "Running batch: 95 of total 307\n",
      "Running batch: 96 of total 307\n",
      "Running batch: 97 of total 307\n",
      "Running batch: 98 of total 307\n",
      "Running batch: 99 of total 307\n",
      "Running batch: 100 of total 307\n",
      "Running batch: 101 of total 307\n",
      "Running batch: 102 of total 307\n",
      "Running batch: 103 of total 307\n",
      "Running batch: 104 of total 307\n",
      "Running batch: 105 of total 307\n",
      "Running batch: 106 of total 307\n",
      "Running batch: 107 of total 307\n",
      "Running batch: 108 of total 307\n",
      "Running batch: 109 of total 307\n",
      "Running batch: 110 of total 307\n",
      "Running batch: 111 of total 307\n",
      "Running batch: 112 of total 307\n",
      "Running batch: 113 of total 307\n",
      "Running batch: 114 of total 307\n",
      "Running batch: 115 of total 307\n",
      "Running batch: 116 of total 307\n",
      "Running batch: 117 of total 307\n",
      "Running batch: 118 of total 307\n",
      "Running batch: 119 of total 307\n",
      "Running batch: 120 of total 307\n",
      "Running batch: 121 of total 307\n",
      "Running batch: 122 of total 307\n",
      "Running batch: 123 of total 307\n",
      "Running batch: 124 of total 307\n",
      "Running batch: 125 of total 307\n",
      "Running batch: 126 of total 307\n",
      "Running batch: 127 of total 307\n",
      "Running batch: 128 of total 307\n",
      "Running batch: 129 of total 307\n",
      "Running batch: 130 of total 307\n",
      "Running batch: 131 of total 307\n",
      "Running batch: 132 of total 307\n",
      "Running batch: 133 of total 307\n",
      "Running batch: 134 of total 307\n",
      "Running batch: 135 of total 307\n",
      "Running batch: 136 of total 307\n",
      "Running batch: 137 of total 307\n",
      "Running batch: 138 of total 307\n",
      "Running batch: 139 of total 307\n",
      "Running batch: 140 of total 307\n",
      "Running batch: 141 of total 307\n",
      "Running batch: 142 of total 307\n",
      "Running batch: 143 of total 307\n",
      "Running batch: 144 of total 307\n",
      "Running batch: 145 of total 307\n",
      "Running batch: 146 of total 307\n",
      "Running batch: 147 of total 307\n",
      "Running batch: 148 of total 307\n",
      "Running batch: 149 of total 307\n",
      "Running batch: 150 of total 307\n",
      "Running batch: 151 of total 307\n",
      "Running batch: 152 of total 307\n",
      "Running batch: 153 of total 307\n",
      "Running batch: 154 of total 307\n",
      "Running batch: 155 of total 307\n",
      "Running batch: 156 of total 307\n",
      "Running batch: 157 of total 307\n",
      "Running batch: 158 of total 307\n",
      "Running batch: 159 of total 307\n",
      "Running batch: 160 of total 307\n",
      "Running batch: 161 of total 307\n",
      "Running batch: 162 of total 307\n",
      "Running batch: 163 of total 307\n",
      "Running batch: 164 of total 307\n",
      "Running batch: 165 of total 307\n",
      "Running batch: 166 of total 307\n",
      "Running batch: 167 of total 307\n",
      "Running batch: 168 of total 307\n",
      "Running batch: 169 of total 307\n",
      "Running batch: 170 of total 307\n",
      "Running batch: 171 of total 307\n",
      "Running batch: 172 of total 307\n",
      "Running batch: 173 of total 307\n",
      "Running batch: 174 of total 307\n",
      "Running batch: 175 of total 307\n",
      "Running batch: 176 of total 307\n",
      "Running batch: 177 of total 307\n",
      "Running batch: 178 of total 307\n",
      "Running batch: 179 of total 307\n",
      "Running batch: 180 of total 307\n",
      "Running batch: 181 of total 307\n",
      "Running batch: 182 of total 307\n",
      "Running batch: 183 of total 307\n",
      "Running batch: 184 of total 307\n",
      "Running batch: 185 of total 307\n",
      "Running batch: 186 of total 307\n",
      "Running batch: 187 of total 307\n",
      "Running batch: 188 of total 307\n",
      "Running batch: 189 of total 307\n",
      "Running batch: 190 of total 307\n",
      "Running batch: 191 of total 307\n",
      "Running batch: 192 of total 307\n",
      "Running batch: 193 of total 307\n",
      "Running batch: 194 of total 307\n",
      "Running batch: 195 of total 307\n",
      "Running batch: 196 of total 307\n",
      "Running batch: 197 of total 307\n",
      "Running batch: 198 of total 307\n",
      "Running batch: 199 of total 307\n",
      "Running batch: 200 of total 307\n",
      "Running batch: 201 of total 307\n",
      "Running batch: 202 of total 307\n",
      "Running batch: 203 of total 307\n",
      "Running batch: 204 of total 307\n",
      "Running batch: 205 of total 307\n",
      "Running batch: 206 of total 307\n",
      "Running batch: 207 of total 307\n",
      "Running batch: 208 of total 307\n",
      "Running batch: 209 of total 307\n",
      "Running batch: 210 of total 307\n",
      "Running batch: 211 of total 307\n",
      "Running batch: 212 of total 307\n",
      "Running batch: 213 of total 307\n",
      "Running batch: 214 of total 307\n",
      "Running batch: 215 of total 307\n",
      "Running batch: 216 of total 307\n",
      "Running batch: 217 of total 307\n",
      "Running batch: 218 of total 307\n",
      "Running batch: 219 of total 307\n",
      "Running batch: 220 of total 307\n",
      "Running batch: 221 of total 307\n",
      "Running batch: 222 of total 307\n",
      "Running batch: 223 of total 307\n",
      "Running batch: 224 of total 307\n",
      "Running batch: 225 of total 307\n",
      "Running batch: 226 of total 307\n",
      "Running batch: 227 of total 307\n",
      "Running batch: 228 of total 307\n",
      "Running batch: 229 of total 307\n",
      "Running batch: 230 of total 307\n",
      "Running batch: 231 of total 307\n",
      "Running batch: 232 of total 307\n",
      "Running batch: 233 of total 307\n",
      "Running batch: 234 of total 307\n",
      "Running batch: 235 of total 307\n",
      "Running batch: 236 of total 307\n",
      "Running batch: 237 of total 307\n",
      "Running batch: 238 of total 307\n",
      "Running batch: 239 of total 307\n",
      "Running batch: 240 of total 307\n",
      "Running batch: 241 of total 307\n",
      "Running batch: 242 of total 307\n",
      "Running batch: 243 of total 307\n",
      "Running batch: 244 of total 307\n",
      "Running batch: 245 of total 307\n",
      "Running batch: 246 of total 307\n",
      "Running batch: 247 of total 307\n",
      "Running batch: 248 of total 307\n",
      "Running batch: 249 of total 307\n",
      "Running batch: 250 of total 307\n",
      "Running batch: 251 of total 307\n",
      "Running batch: 252 of total 307\n",
      "Running batch: 253 of total 307\n",
      "Running batch: 254 of total 307\n",
      "Running batch: 255 of total 307\n",
      "Running batch: 256 of total 307\n",
      "Running batch: 257 of total 307\n",
      "Running batch: 258 of total 307\n",
      "Running batch: 259 of total 307\n",
      "Running batch: 260 of total 307\n",
      "Running batch: 261 of total 307\n",
      "Running batch: 262 of total 307\n",
      "Running batch: 263 of total 307\n",
      "Running batch: 264 of total 307\n",
      "Running batch: 265 of total 307\n",
      "Running batch: 266 of total 307\n",
      "Running batch: 267 of total 307\n",
      "Running batch: 268 of total 307\n",
      "Running batch: 269 of total 307\n",
      "Running batch: 270 of total 307\n",
      "Running batch: 271 of total 307\n",
      "Running batch: 272 of total 307\n",
      "Running batch: 273 of total 307\n",
      "Running batch: 274 of total 307\n",
      "Running batch: 275 of total 307\n",
      "Running batch: 276 of total 307\n",
      "Running batch: 277 of total 307\n",
      "Running batch: 278 of total 307\n",
      "Running batch: 279 of total 307\n",
      "Running batch: 280 of total 307\n",
      "Running batch: 281 of total 307\n",
      "Running batch: 282 of total 307\n",
      "Running batch: 283 of total 307\n",
      "Running batch: 284 of total 307\n",
      "Running batch: 285 of total 307\n",
      "Running batch: 286 of total 307\n",
      "Running batch: 287 of total 307\n",
      "Running batch: 288 of total 307\n",
      "Running batch: 289 of total 307\n",
      "Running batch: 290 of total 307\n",
      "Running batch: 291 of total 307\n",
      "Running batch: 292 of total 307\n",
      "Running batch: 293 of total 307\n",
      "Running batch: 294 of total 307\n",
      "Running batch: 295 of total 307\n",
      "Running batch: 296 of total 307\n",
      "Running batch: 297 of total 307\n",
      "Running batch: 298 of total 307\n",
      "Running batch: 299 of total 307\n",
      "Running batch: 300 of total 307\n",
      "Running batch: 301 of total 307\n",
      "Running batch: 302 of total 307\n",
      "Running batch: 303 of total 307\n",
      "Running batch: 304 of total 307\n",
      "Running batch: 305 of total 307\n",
      "Running batch: 306 of total 307\n",
      "Running batch: 307 of total 307\n",
      "Started compute metrics\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.90      3368\n",
      "           1       0.83      0.88      0.85      3219\n",
      "           2       0.92      0.89      0.90      3237\n",
      "\n",
      "    accuracy                           0.88      9824\n",
      "   macro avg       0.88      0.88      0.88      9824\n",
      "weighted avg       0.88      0.88      0.88      9824\n",
      "\n",
      "Test Rouge scores: {'rouge1': 48.9872, 'rouge2': 23.9398, 'rougeL': 43.7372, 'rougeLsum': 43.9183, 'gen_len': 16.3543}\n"
     ]
    }
   ],
   "source": [
    "# Make reproducible as much as possible\n",
    "np.random.seed(1234)\n",
    "#tf.random.set_seed(1234)\n",
    "python_random.seed(1234)\n",
    "seed = 1234\n",
    "\n",
    "\n",
    "\n",
    "# Values calculated in the data-explore notebook\n",
    "max_input_length = 125\n",
    "max_output_length = 193\n",
    "\n",
    "\n",
    "def preprocess(data):\n",
    "    \"\"\"\n",
    "    Function used to preprocess the input and output for the model.\n",
    "    The input gets the special [SEP] token and the output gets a space added.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"input\": data['premise'] + '</s>' + data['hypothesis'],\n",
    "        \"output\": str(data['label']) + ' ' + data['explanation_1'],\n",
    "    }\n",
    "\n",
    "# adapted from:\n",
    "# https://github.com/huggingface/transformers/blob/main/examples/pytorch/summarization/run_summarization.py\n",
    "def batch_tokenize_preprocess(batch, tokenizer, max_source_length, max_target_length):\n",
    "    \"\"\"\n",
    "    Function used to tokenize our inputs in batches, saves RAM usage/\n",
    "    \"\"\"\n",
    "    input_tokenized = tokenizer(\n",
    "        batch['input'], padding=\"max_length\", truncation=True, max_length=max_source_length\n",
    "    )\n",
    "\n",
    "    output_tokenized = tokenizer(\n",
    "        batch['output'], padding=\"max_length\", truncation=True, max_length=max_target_length\n",
    "    )\n",
    "\n",
    "    batch = {k: v for k, v in input_tokenized.items()}\n",
    "\n",
    "    # Ignore padding in the loss\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in l]\n",
    "        for l in output_tokenized[\"input_ids\"]\n",
    "    ]\n",
    "    return batch\n",
    "\n",
    "\n",
    "\n",
    "# Adapted from:\n",
    "# https://github.com/huggingface/transformers/blob/main/examples/pytorch/summarization/run_summarization.py\n",
    "rouge_metric = datasets.load_metric(\"rouge\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    \"\"\"\n",
    "    Function that is used to reformat the data in a way that we can create a classification report\n",
    "    and obtain the ROUGE scores\n",
    "    \"\"\"\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in labels]\n",
    "\n",
    "    # Code that is needed to get the classification report\n",
    "    nli_preds, nli_golds = [], []\n",
    "    correct_labels = ['0', '1', '2']\n",
    "    for idx, l in enumerate(labels):\n",
    "      n = preds[idx][0]\n",
    "      if n in correct_labels:\n",
    "        nli_preds.append(n)\n",
    "      else:\n",
    "        nli_preds.append('NA')\n",
    "      nli_golds.append(l[0])\n",
    "\n",
    "    return preds, labels, nli_preds, nli_golds\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"\n",
    "    Function that is used to obtain the ROUGE and classifcation report\n",
    "    \"\"\"\n",
    "   # preds = the output of the model that was generated\n",
    "   # labels = gold labels from the validation set\n",
    "    print('Started compute metrics')\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # take care of padding\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels, nli_preds, nli_golds = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    print(classification_report(nli_golds, nli_preds))\n",
    "    \n",
    "    rouge = rouge_metric.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "\n",
    "    # Extract a few results from ROUGE\n",
    "    rouge = {key: value.mid.fmeasure * 100 for key, value in rouge.items()}\n",
    "\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    rouge[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    rouge = {k: round(v, 4) for k, v in rouge.items()}\n",
    "    return rouge\n",
    "\n",
    "\n",
    "def generate_predictions(model, test_dict):\n",
    "    \"\"\"\n",
    "    Function that is used to obtain the test predictions and metrics\n",
    "    \"\"\"\n",
    "\n",
    "    test_data = test_dict.map(\n",
    "        lambda batch: batch_tokenize_preprocess(\n",
    "            batch, tokenizer, max_input_length, max_output_length\n",
    "        ),\n",
    "        batched=True,\n",
    "        remove_columns=test_dict.column_names,\n",
    "    )\n",
    "    test_data.set_format(\"torch\")\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        test_data, batch_size=32)\n",
    "    # Use the trained model to generate outputs using the input ids and AM\n",
    "    outputs = []\n",
    "\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        print(f'Running batch: {idx + 1} of total {len(dataloader)}')\n",
    "\n",
    "        # Obtain iput ids and attention mask\n",
    "        input_ids = batch[\"input_ids\"].to(model.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "\n",
    "        outputs.append(model.generate(input_ids, attention_mask=attention_mask))\n",
    "\n",
    "    test_outputs = torch.cat(outputs, dim=0)\n",
    "    print('Test Rouge scores:', compute_metrics((test_outputs.cpu().detach(), test_data[\"labels\"].cpu().detach())))\n",
    "\n",
    "    # Coverts output_ids back to string representation using the decode\n",
    "    output_str = tokenizer.batch_decode(test_outputs, skip_special_tokens=True)\n",
    "    \n",
    "    bart_scores = []\n",
    "    for idx, generation in enumerate(output_str):\n",
    "      bart_score = bart_scorer.score([generation], [test_dict['output'][idx]])[0]\n",
    "\n",
    "      bart_scores.append((generation, test_dict['output'][idx], bart_score))\n",
    "  \n",
    "    return output_str, bart_scores\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function of the script!\n",
    "    \"\"\"\n",
    "\n",
    "    test = datasets.load_dataset('esnli', split='test').shuffle(seed)\n",
    "\n",
    "    test_dict = test.map(preprocess, remove_columns=['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2',\n",
    "                                                     'explanation_3'])\n",
    "    \n",
    "    # Evaluate on our test set\n",
    "    predictions_after_tuning, bart_scores = generate_predictions(model, test_dict)\n",
    "\n",
    "    pf = f'pickles/BART_bart_scores.pk'\n",
    "    with open(pf, 'wb') as f:\n",
    "      pickle_rick.dump(bart_scores, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "HtrG3uIUPGot",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1649601488165,
     "user_tz": -120,
     "elapsed": 15,
     "user": {
      "displayName": "F.A. Leistra",
      "userId": "17579912925770168307"
     }
    }
   },
   "execution_count": 13,
   "outputs": []
  }
 ]
}