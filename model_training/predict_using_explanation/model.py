import torch
import os

from transformers import RobertaTokenizerFast, RobertaForSequenceClassification, RobertaForCausalLM, RobertaModel, AutoTokenizer, AutoModelForSeq2SeqLM, get_scheduler

from torch.utils.data import DataLoader
from torch.optim import AdamW

from tqdm.auto import tqdm

from sklearn.metrics import classification_report

from data_analysis import data

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)


def train_bart_classifier(dataset):
    """
    Train a NLI classification model using the explanations generated by the fine-tuned bart model
    """

    bartTokenizer = AutoTokenizer.from_pretrained('./models/bart')
    explModel = AutoModelForSeq2SeqLM.from_pretrained('./models/bart')
    explModel.to(device)

    tokenizer = RobertaTokenizerFast.from_pretrained("roberta-base")

    def tokenize(examples):

        tokens = bartTokenizer(examples["premise_hypothesis"], padding="max_length",
                               max_length=125, truncation=True, return_tensors="pt")

        input_ids = tokens["input_ids"].to(explModel.device)
        attention_mask = tokens["attention_mask"].to(explModel.device)

        expl_outputs = explModel.generate(
            input_ids, attention_mask=attention_mask)

        expl_output_str = tokenizer.batch_decode(
            expl_outputs, skip_special_tokens=True)

        expl_input = [s[1:] for s in expl_output_str]

        processed = tokenizer(expl_input, padding="max_length",
                              max_length=193, truncation=True)

        processed["labels"] = examples["label"]

        return processed

    model = RobertaForSequenceClassification.from_pretrained(
        "roberta-base", num_labels=3)
    model.to(device)

    tokenized_dataset = dataset["train"].map(
        tokenize, batched=True, batch_size=32, remove_columns=dataset["train"].column_names)
    tokenized_dataset.set_format(
        "torch", columns=['input_ids', 'attention_mask', 'labels'])

    train_dataloader = DataLoader(
        tokenized_dataset, shuffle=True, batch_size=32)

    optimizer = AdamW(model.parameters(), lr=5e-5)

    num_epochs = 3

    num_training_steps = num_epochs * len(train_dataloader)

    lr_scheduler = get_scheduler(
        name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps
    )

    progress_bar = tqdm(range(num_training_steps))

    model.train()

    for epoch in range(num_epochs):
        for batch in train_dataloader:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            loss = outputs.loss
            loss.backward()

            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            progress_bar.update(1)

    __check_dir_exists("models")

    model.save_pretrained("./models/classifier_bart_expl")

    evaluate_classifier(dataset, "./models/classifier_bart_expl")


def train_classifier(dataset):
    """
    Train the a NLI classification model using the human-annotated explanations as input
    """

    tokenized_dataset = __classifier_tokenize(dataset["train"])

    train_dataloader = DataLoader(
        tokenized_dataset, shuffle=True, batch_size=32)

    model = RobertaForSequenceClassification.from_pretrained(
        "roberta-base", num_labels=3)
    model.to(device)

    optimizer = AdamW(model.parameters(), lr=5e-5)

    num_epochs = 3

    num_training_steps = num_epochs * len(train_dataloader)

    lr_scheduler = get_scheduler(
        name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps
    )

    progress_bar = tqdm(range(num_training_steps))

    model.train()

    for epoch in range(num_epochs):
        for batch in train_dataloader:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            loss = outputs.loss
            loss.backward()

            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            progress_bar.update(1)

    __check_dir_exists("models")

    model.save_pretrained("./models/classifier_expl_only")

    evaluate_classifier(dataset, "./models/classifier_expl_only")


def evaluate_classifier(dataset, model_path):
    """
    Evaluate the classification model
    """

    tokenized_dataset = __classifier_tokenize(dataset["test"])

    eval_dataloader = DataLoader(
        tokenized_dataset, batch_size=32)

    model = RobertaForSequenceClassification.from_pretrained(
        model_path, num_labels=3)

    model.to(device)

    model.eval()

    progress_bar = tqdm(range(len(eval_dataloader)))

    nli_pred = []
    nli_gold = []

    for batch in eval_dataloader:

        batch = {k: v.to(device) for k, v in batch.items()}

        with torch.no_grad():

            outputs = model(**batch)

        logits = outputs.logits

        nli_pred.extend(torch.argmax(logits, dim=-1).tolist())

        nli_gold.extend(batch["labels"])

        progress_bar.update(1)

    print(classification_report(y_pred=nli_pred, y_true=nli_gold,
          target_names=["entailment", "neutral", "contradiction"]))


def __classifier_tokenize(dataset):

    tokenizer = RobertaTokenizerFast.from_pretrained("roberta-base")

    def tokenize(examples):
        tokens = tokenizer(examples["explanation_1"],
                           padding="max_length", max_length=250, truncation=True)
        tokens["labels"] = examples["label"]
        return tokens

    tokenized_dataset = dataset.map(
        tokenize, batched=True, num_proc=4, remove_columns=dataset.column_names)
    tokenized_dataset.set_format(
        "torch", columns=['input_ids', 'attention_mask', 'labels'])

    return tokenized_dataset


def __check_dir_exists(dir: str):
    path = str(dir)
    if not os.path.exists(path):
        os.makedirs(path)


if __name__ == "__main__":
    ds = data.load_data()

    train_classifier(ds)

    train_bart_classifier(ds)
